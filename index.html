<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Fangfu Liu</title>
  
  <meta name="author" content="Fangfu Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Fangfu Liu | 刘芳甫</name>
              </p>
              <p> 
                I'm a second-year PhD student in the Department of <a href="https://www.ee.tsinghua.edu.cn/en/"> Electronic Engineering </a> at <a href="https://www.tsinghua.edu.cn/en/"> Tsinghua University </a>, advised by Prof. <a href="https://duanyueqi.github.io/">Yueqi Duan</a>. In 2023, I obtained my B.Eng. in the Department of Electronic Engineering, Tsinghua University. 
              </p>
              <p>
                My research interest lies in <b>Machine Learning</b> (e.g., Causal Learning) and <b>Computer Vision (e.g., 3D AIGC and Video Generation)</b>. I aim to build spatially intelligent AI that can model the world and reason about objects, places, and interactions in 3D space and time.
              </p>
	      <b>If you are interested in working with us (in person or remotely) as an intern at Tsinghua University, please feel free to drop me an email.</b>
              <p style="text-align:center">
                <a href="mailto:fangfu19@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://liuff19.github.io/">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=b-4FUVsAAAAJ&hl=zh-CN&oi=ao"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/liuff19"> Github </a> &nbsp/&nbsp
		<a href="https://x.com/fangfu0830"> Twitter </a>
              </p>
            </td>
            <td style="padding:3%;width:40%;max-width:40%">
              <img style="width:70%;max-width:70%" alt="profile photo" src="images/graduate.jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
	      <li style="margin: 5px;" >
              <b>2025-02:</b> Two papers on 4D Dynamics and Video Generation are accepted by <a href="https://cvpr.thecvf.com/">CVPR 2025</a>.
              </li>
	      <li style="margin: 5px;" >
              <b>2024-09:</b> Two papers on 3D Vision are accepted by <a href="https://neurips.cc/">NeurIPS 2024</a>.
              </li>
	      <li style="margin: 5px;" >
              <b>2024-07:</b> Three papers on 3D AIGC are accepted by <a href="https://eccv.ecva.net/">ECCV 2024</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-02:</b> One paper on 3D AIGC is accepted by <a href="https://cvpr.thecvf.com/">CVPR 2024</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-05:</b> One paper on Structure Learning is accepted by <a href="https://kdd.org/kdd2023/">KDD 2023</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-02:</b> One paper on NeRF is accepted by <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-01:</b> One paper on Causal Discovery is accepted by <a href="https://iclr.cc/">ICLR 2023</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Preprints</heading></p>
              <p>
                *Equal contribution &nbsp;&nbsp; <sup>†</sup>Project leader
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/pipeline-dimensionX.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DimensionX: Create Any 3D and 4D Scenes from a Single Image with Controllable Video Diffusion</papertitle>
              <br>
              <a href="https://github.com/wenqsun"> Wenqiang Sun* </a>,
              <a href="https://chenshuo20.github.io/"> Shuo Chen* </a>,
              <strong>Fangfu Liu*</strong>,
              <a href="https://scholar.google.com/citations?user=2pbka1gAAAAJ"> Zilong Chen </a>,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
              <a href="https://eejzhang.people.ust.hk/home.html"> Jun Zhang </a>,
              <a href="https://yikaiw.github.io/"> Yikai Wang </a>
              <br>
              <em>Arxiv, 2024</em>
              <br>
              <a href="https://arxiv.org/abs/2411.04928">[arXiv]</a>
              <a href="https://github.com/wenqsun/DimensionX">[Code]</a>
              <a href="https://chenshuo20.github.io/DimensionX/">[Project Page]</a> 
              <br>
              <p> In this paper, we introduce DimensionX, a framework designed to generate photorealistic 3D and 4D scenes from just a single image with video diffusion. We believe that our research provides a promising direction to create a dynamic and interactive environment with video diffusion models.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/ReconX.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model</papertitle>
              <br>
	            <strong>Fangfu Liu*</strong>,
              <a href="https://github.com/wenqsun"> Wenqiang Sun* </a>,
              <a href="https://hanyang-21.github.io/"> Hanyang Wang* </a>,
              <a href="https://yikaiw.github.io/"> Yikai Wang </a>,
              <a href="https://github.com/sunboy0617"> Haowen Sun </a>,
              <a href="https://jamesyjl.github.io/"> Junliang Ye </a>,
              <br>
              <a href="https://eejzhang.people.ust.hk/home.html"> Jun Zhang </a>,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>
              <br>
              <em>Arxiv, 2024</em>
              <br>
              <a href="https://arxiv.org/abs/2408.16767">[arXiv]</a>
              <a href="https://github.com/liuff19/ReconX">[Code]</a>
              <a href="https://liuff19.github.io/ReconX/">[Project Page]</a> 
              <br>
              <p> In this paper, we propose ReconX, a novel 3D scene reconstruction paradigm that reframes the ambiguous reconstruction challenge as a temporal generation task. The key insight is to unleash the strong generative prior of large pre-trained video diffusion models for sparse-view reconstruction. </p>
            </td>
          </tr>
	  <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DreamCinema.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DreamCinema: Cinematic Transfer with Free Camera and 3D Character</papertitle>
              <br>
	      Weiliang Chen,
              <strong>Fangfu Liu<sup>†</sup></strong>, 
              Diankun Wu,
	      Haowen Sun,
	      Haixu Song,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>
              <br>
              <em>Arxiv, 2024</em>
              <br>
              <a href="https://www.arxiv.org/abs/2408.12601">[arXiv]</a>
              <a href="https://github.com/chen-wl20/DreamCinema">[Code]</a>
              <a href="https://liuff19.github.io/DreamCinema/">[Project Page]</a> 
              <br>
              <p> In this paper, we propose DreamCinema, a novel cinematic transfer framework that pioneers generative AI into the film production paradigm, aiming at facilitating user-friendly film creation. </p>
            </td>
          </tr>
		
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Physics3D.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion</papertitle>
              <br>
              <strong>Fangfu Liu*</strong>, 
              Hanyang Wang*
              <a href="https://scholar.google.com/citations?user=i4kyLbwAAAAJ"> Shunyu Yao </a>,
              Shengjun Zhang,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ"> Jie Zhou</a>,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>
              <br>
              <em>Arxiv, 2024</em>
              <br>
              <a href="https://arxiv.org/abs/2406.04338">[arXiv]</a>
              <a href="https://github.com/liuff19/Physics3D">[Code]</a>
              <a href="https://liuff19.github.io/Physics3D">[Project Page]</a> 
              <br>
              <p> In this paper, we propose Physics3D, a novel method for learning various physical properties of 3D objects through a video diffusion model. Our approach involves designing a highly generalizable physical simulation system based on a viscoelastic material model, which enables us to simulate a wide range of materials with high-fidelity capabilities. </p>
            </td>
          </tr>

        </tbody></table>
		
	<!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Selected Publications</heading></p>
              <p>
                *Equal contribution &nbsp;&nbsp; <sup>†</sup>Project leader
              </p>
            </td>
          </tr>
        </tbody></table> -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <p><heading>Selected Publications</heading></p>
            <p>
              *Equal contribution &nbsp;&nbsp; <sup>†</sup>Project leader
            </p>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Unique3d.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=VTU0gysAAAAJ&hl=zh-CN&oi=ao"> Kailu Wu </a>, 
              <strong>Fangfu Liu</strong>, 
              Zhihan Cai, 
              Runjie Yan, 
              Hanyang Wang, 
              Yating Hu,
              <br>
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
              <a href="https://group.iiis.tsinghua.edu.cn/~maks/"> Kaisheng Ma </a>
              <br>
              <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2405.20343">[arXiv]</a>
              <a href="https://github.com/AiuniAI/Unique3D">[Code]</a>
              <a href="https://wukailu.github.io/Unique3D/">[Project Page]</a> 
              <br>
              <p> In this work, we introduce Unique3D, a novel image-to-3D framework for efficiently generating high-quality 3D meshes from single-view images, featuring state-of-the-art generation fidelity and strong generalizability. Unique3D can generate a high-fidelity textured mesh from a single orthogonal RGB image of any object in under 30 seconds.  </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GGN.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images</papertitle>
              <br>
              <a href="https://shengjun-zhang.github.io/"> Shengjun Zhang </a>, 
              Xin Fei,
              <strong>Fangfu Liu</strong>, 
              Haixu Song,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>
              <br>
              <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2503.16338">[arXiv]</a>
              <a href="https://github.com/shengjun-zhang/GGN">[Code]</a>
              <a href="https://shengjun-zhang.github.io/GGN/">[Project Page]</a> 
              <br>
              <p>In this paper, we propose Gaussian Graph Network (GGN) to generate efficient and generalizable Gaussian representations. Specifically, we construct Gaussian Graphs to model the relations of Gaussian groups from different views. Compared to the state-of-the-art methods, our model uses fewer Gaussians and achieves better image quality with higher rendering speed. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DreamReward.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DreamReward: Text-to-3D Generation with Human Preference</papertitle>
              <br>
              <a href="https://jamesyjl.github.io/"> Junliang Ye* </a>, 
              <strong>Fangfu Liu*</strong>, 
              Qixiu Li,
              <a href="https://thuwzy.github.io/"> Zhengyi Wang </a>,
              <a href="https://yikaiw.github.io/"> Yikai Wang </a>,
              <br>
              Xinzhou Wang,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
              <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml"> Jun Zhu </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2403.14613">[arXiv]</a>
              <a href="https://github.com/liuff19/DreamReward">[Code]</a>
              <a href="https://jamesyjl.github.io/DreamReward/">[Project Page]</a> 
              <br>
              <p> In this work, We propose the first general-purpose human preference reward model for text-to-3D generation, named Reward3D. Then we further introduce a novel text-to-3D framework, coined DreamReward, which greatly boosts high-text alignment and high-quality 3D generation through human preference feedback. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/makeyour3d.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation</papertitle>
              <br>
              <strong>Fangfu Liu</strong>, 
              Hanyang Wang, 
              Weiliang Chen,
              Haowen Sun,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2403.09625">[arXiv]</a>
              <a href="https://github.com/liuff19/Make-Your-3D">[Code]</a>
              <a href="https://liuff19.github.io/Make-Your-3D/">[Project Page]</a> 
              <br>
              <p> We introduce a novel 3D customization method, dubbed Make-Your-3D that can personalize high-fidelity and consistent 3D content from only a single image of a subject with text description within 5 
                  minutes. </p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Sherpa.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior</papertitle>
              <br>
              <strong>Fangfu Liu</strong>, 
              Diankun Wu, 
              <a href="https://weiyithu.github.io/"> Yi Wei </a>, 
              <a href="https://raoyongming.github.io/"> Yongming Rao </a>,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2312.06655">[arXiv]</a>
              <a href="https://github.com/liuff19/Sherpa3D">[Code]</a>
              <a href="https://liuff19.github.io/Sherpa3D/">[Project Page]</a> 
              <br>
              <p> We propose Sherpa3D, a new text-to-3D framework that achieves high-fidelity, generalizability, and geometric consistency simultaneously. Extensive experiments show the superiority of our Sherpa3D over the state-of-the-art text-to-3D methods in terms of quality and 3D consistency.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/CASPER.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Discovering Dynamic Causal Space for DAG Structure Learning</papertitle>
              <br>
              <strong>Fangfu Liu</strong>, 
              Wenchang Ma, 
              <a href="https://anzhang314.github.io/index.html"> An Zhang </a>, 
              <a href="https://xiangwang1223.github.io/"> Xiang Wang </a>,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
              <a href="https://scholar.google.com/citations?user=Z9DWCBEAAAAJ&hl=en&oi=ao"> Tat-Seng Chua </a>
              <br>
              <em>ACM SIGKDD Conference on Knowledge Discovery and Data Mining (<strong>KDD</strong>)</em>, 2023
              <br>
              <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2306.02822">[arXiv]</a>
              <a href="https://arxiv.org/abs/2306.02822">[Code]</a>
              <a href="https://arxiv.org/abs/2306.02822">[Project Page]</a> 
              <br>
              <p> we propose a dynamic causal space for DAG structure learning, coined CASPER, that integrates the graph structure into the score function as a new measure in the causal space to faithfully reflect the causal distance between estimated and groundtruth DAG.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/S-Ray.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Semantic Ray: Learning a Generalizable Semantic Field with Cross-Reprojection Attention</papertitle>
              <br>
              <strong>Fangfu Liu</strong>, 
              Chubin Zhang, 
              <a href="https://yzheng97.github.io/"> Yu Zheng</a>, 
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.13014">[arXiv]</a>
              <a href="https://github.com/liuff19/Semantic-Ray">[Code]</a>
              <a href="https://liuff19.github.io/S-Ray/">[Project Page]</a> 
              <br>
              <p> We propose a neural semantic representation called Semantic-Ray (S-Ray) to build a generalizable semantic field, which is able to learn from multiple scenes and directly infer semantics on novel viewpoints across novel scenes.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/rescore.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Boosting Differentiable Causal Discovery via Adaptive Sample Reweighting</papertitle>
              <br>
              <a href="https://anzhang314.github.io/index.html"> An Zhang</a>,
              <strong>Fangfu Liu</strong>, 
              Wenchang Ma,
              Zhibo Cai,
              <a href="https://xiangwang1223.github.io/"> Xiang Wang </a>,
              <a href="https://scholar.google.com/citations?user=Z9DWCBEAAAAJ&hl=en&oi=ao"> Tat-Seng Chua </a>
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.03187">[arXiv]</a>
              <a href="https://github.com/anzhang314/ReScore">[Code]</a>
              <a href="https://arxiv.org/abs/2303.03187">[Project Page]</a> 
              <br>
              <p> We propose ReScore, a simple-yet-effective model-agnostic optimzation framework that simultaneously eliminates spurious edge learning and generalizes to heterogeneous data by utilizing learnable adaptive weights.</p>
            </td>
          </tr>

<!--           <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/VL-grasp.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>VL-Grasp: a 6-Dof Interactive Grasp Policy for Language-Oriented Objects in Cluttered Indoor Scenes</papertitle>
              <br>
              Yuhao Lu,
              Yixuan Fan,
              Beixing Deng,
              <strong>Fangfu Liu</strong>, 
              Yali Li,
              <a href="https://scholar.google.com/citations?user=RgzLZZsAAAAJ&hl=en&oi=ao"> Shengjin Wang </a>
              <br>
              <em>International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2308.00640">[arXiv]</a>
              <a href="https://github.com/luyh20/VL-Grasp">[Code]</a>
              <a href="https://arxiv.org/abs/2308.00640">[Project Page]</a> 
              <br>
              <p> The VL-Grasp is an interactive grasp policy combined with visual grounding and 6-dof grasp pose detection tasks. The robot can adapt to various observation views and more diverse indoor scenes to grasp the target according to a human's language command by applying the VL-Grasp. Meanwhile, we build a new visual grounding dataset specially designed for the robot interaction grasp task, called RoboRefIt.</p>
            </td>
          </tr> -->



        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> <b>National Scholarship (Top 1 in 260+ in the 2019-2020 academic year)</b></li>
                <li style="margin: 5px;"> Tsinghua University Comprehensive Excellent Award <b>twice (Top 5% in 260+, 2020&2021)</b></li>
                <li style="margin: 5px;"> Tsinghua Science and Technology Innovation Excellence Award (2022)</li>
                <li style="margin: 5px;"> Four star Bauhinia volunteer of Tsinghua University (Volunteer hours up to 150, 2021)</li>
                <li style="margin: 5px;"> Advanced individual award of Tsinghua University (2019)</li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                Review for <b>TCSVT</b>, <b>CVPR</b>, <b>NeurIPS</b>, <b>IROS</b>, <b>ACMMM</b>, <b>ICML</b>, <b>ICLR</b>, <b>ICCV</b>.
              </li>
          </td>
        </tr>
      </tbody></table>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
      <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=B_hoqUcZkAVteexiuKv_tIvNw9enA1g2tIC3ypxXP2E"></script>
	  <br>
	    &copy; Fangfu Liu | Last updated: Mar, 2025
</center></p>
</body>

</html>
