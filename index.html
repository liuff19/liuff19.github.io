<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Fangfu Liu</title>
  
  <meta name="author" content="Fangfu Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Fangfu Liu | 刘芳甫</name>
              </p>
              <p> 
                I am a senior undergeraduate student majoring in <a href="https://www.ee.tsinghua.edu.cn/en/"> Electronic Engineering </a> at <a href="https://www.tsinghua.edu.cn/en/"> Tsinghua University </a>. My research interest lies at the causality, machine learning and 3D computer vision. 
              </p>
              <p style="text-align:center">
                <a href="mailto:liuff19@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <a href="files/CV_fangfuliu.pdf">CV</a> &nbsp/&nbsp
                <a href="liuff19.github.io"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/liuff19"> Github </a>
              </p>
            </td>
            <td style="padding:3%;width:40%;max-width:40%">
              <img style="width:60%;max-width:60%" alt="profile photo" src="images/fangfu(1).jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2023-02:</b> One paper on NeRF is accepted by <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-01:</b> One paper on Causal Discovery is accepted by <a href="https://iclr.cc/">ICLR 2023</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Publications</heading></p>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/hornet.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Semantic Ray: Learning a Generalizable Semantic Field with Cross-Reprojection Attention</papertitle>
              <br>
              <strong>Fangfu Liu</strong>, 
              Chubin Zhang, 
              <a href="https://yzheng97.github.io/"> Yu Zheng</a>, 
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2303.13014">[arXiv]</a>
              <a href="https://github.com/liuff19/Semantic-Ray">[Code]</a>
              <a href="liuff19.github.io/S-Ray/">[Project Page]</a> 
              <br>
              <p> HorNet is a family of generic vision backbones that perform explicit high-order spatial interactions based on Recursive Gated Convolution.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/p2p.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with Point-to-Pixel Prompting</papertitle>
              <br>
              <a href="https://wangzy22.github.io/">Ziyi Wang</a>*,
              <a href="https://yuxumin.github.io/">Xumin Yu</a>*,
              <strong>Yongming Rao</strong>*, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022
              <br>
              <a href="http://arxiv.org/abs/2208.02812">[arXiv]</a>
              <a href="https://github.com/wangzy22/P2P">[Code]</a>
              <a href="https://p2p.ivg-research.xyz/">[Project Page]</a> 
              <a href="https://zhuanlan.zhihu.com/p/558286235">[中文解读]</a> 
              <br>
              <p> P2P is a framework to leverage large-scale pre-trained image models for 3D point cloud analysis. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/denseclip_small.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting</papertitle>
              <br>
              <strong>Yongming Rao</strong>*, 
              <a href="https://wl-zhao.github.io/"> Wenliang Zhao</a>*, 
              <a href="https://chengy12.github.io/"> Guangyi Chen</a>, 
              <a href="https://andytang15.github.io/"> Yansong Tang</a>, 
              <a href="http://www.zhengzhu.net/"> Zheng Zhu </a>, 
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2112.01518">[arXiv]</a>
              <a href="https://github.com/raoyongming/DenseCLIP">[Code]</a>
              <a href="https://denseclip.ivg-research.xyz">[Project Page]</a> 
              <a href="https://mp.weixin.qq.com/s/fERXjGBVXzo6TaYiV2Z9ZQ">[中文解读]</a> 
              <br>
              <p> DenseCLIP is a new framework for dense prediction by implicitly and explicitly leveraging the pre-trained knowledge from CLIP.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/pointbert.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Point-BERT: Pre-Training 3D Point Cloud Transformers with Masked Point Modeling</papertitle>
              <br>
              <a href="https://yuxumin.github.io/">Xumin Yu</a>*,
              <a href="https://github.com/lulutang0608">Lulu Tang</a>*,
              <strong>Yongming Rao</strong>*, 
              <a href="http://www.ai.pku.edu.cn/info/1139/1243.htm"> Tiejun Huang</a>,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, 
              <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2111.14819">[arXiv]</a>
              <a href="https://github.com/lulutang0608/Point-BERT">[Code]</a>
              <a href="https://point-bert.ivg-research.xyz/">[Project Page]</a> 
              <a href="https://zhuanlan.zhihu.com/p/484336830">[中文解读]</a> 
              <br>
              <p> Point-BERT is a new paradigm for learning Transformers in an unsupervised manner by generalizing the concept of BERT onto 3D point cloud data.</p>
            </td>
          </tr>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/gfnet_small.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Global Filter Networks for Image Classification</papertitle>
              <br>
              <strong>Yongming Rao</strong>*, <a href="https://wl-zhao.github.io/"> Wenliang Zhao</a>*,   <a href="http://www.zhengzhu.net/"> Zheng Zhu </a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>

              <br>
              <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2107.00645">[arXiv]</a> 
              <a href="https://github.com/raoyongming/GFNet">[Code]</a> 
              <a href="https://gfnet.ivg-research.xyz/">[Project Page]</a> 
              <a href="https://mp.weixin.qq.com/s/-E0jbVfiH6z4vhIIbLv6UA">[中文解读]</a> 
              <br>
              <p> Global Filter Networks is a transformer-style architecture that learns long-term spatial dependencies in the frequency domain with log-linear complexity.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DynamicViT.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification</papertitle>
              <br>
              <strong>Yongming Rao</strong>, <a href="https://wl-zhao.github.io/"> Wenliang Zhao</a>,  <a href="https://liubl1217.github.io/"> Benlin Liu </a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, <a href="http://web.cs.ucla.edu/~chohsieh/"> Cho-Jui Hsieh </a>

              <br>
              <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2106.02034">[arXiv]</a> 
              <a href="https://github.com/raoyongming/DynamicViT">[Code]</a> 
              <a href="https://dynamicvit.ivg-research.xyz/">[Project Page]</a> 
              <a href="https://youtu.be/3gfizSuIf0s">[Video]</a> 
              <a href="https://zhuanlan.zhihu.com/p/379126740">[中文解读]</a> 
              <br>
              <p> We present a dynamic token sparsification framework to prune redundant tokens in vision transformers progressively and dynamically based on the input. </p>
            </td>
          </tr>

            <tr>
              <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/PoinTr.gif" alt="dise">
              </td>
              <td width="75%" valign="center">
                <papertitle>PoinTr: Diverse Point Cloud Completion with Geometry-Aware Transformers</papertitle>
                <br>
                <a href="https://yuxumin.github.io/">Xumin Yu</a>*,  
                <strong>Yongming Rao</strong>*,  Ziyi Wang, Zuyan Liu, 
                <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, 
                <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
  
                <br>
                <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
                <br>
                <font color="red"><strong>Oral Presentation</strong></font>
                <br>
                <a href="https://arxiv.org/abs/2108.08839">[arXiv]</a> 
                <a href="https://github.com/yuxumin/PoinTr/">[Code]</a> 
                <a href="https://zhuanlan.zhihu.com/p/401928647">[中文解读]</a>
                <br>
                <p> PoinTr is a transformer-based framework that reformulates point cloud completion as a set-to-set translation problem. </p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="images/RandomRooms.png" alt="dise">
              </td>
              <td width="75%" valign="center">
                <papertitle> RandomRooms: Unsupervised Pre-training from Synthetic Shapes and Randomized Layouts for 3D Object Detection </papertitle>
                <br>
                <strong>Yongming Rao</strong>*,   <a href="https://liubl1217.github.io/"> Benlin Liu</a>*,  <a href="https://scholar.google.com/citations?user=NrRf7pUAAAAJ&hl=en"> Yi Wei </a>,
                  <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="http://web.cs.ucla.edu/~chohsieh/"> Cho-Jui Hsieh </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
                <br>
                <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
                <br>
                <a href="https://arxiv.org/abs/2108.07794">[arXiv]</a>
                <br>
                <p> We propose to generate random layouts of a scene by
                  making use of the objects in the synthetic CAD dataset and
                  learn the 3D scene representation by applying object-level
                  contrastive learning on two random scenes generated from
                  the same set of synthetic objects. </p>
              </td>
            </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/PointGLR.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Global-Local Bidirectional Reasoning for Unsupervised Representation Learning of 3D Point Clouds</papertitle>
              <br>
              <strong>Yongming Rao</strong>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2003.12971">[arXiv]</a> <a href="https://github.com/raoyongming/PointGLR">[Code]</a>
              <br>
              <p></p>
              <p>We present an unsupervised point cloud representation learning method based on global-local bidirectional reasoning, which largely advances the state-of-the-art of unsupervised point cloud understanding and  <strong>outperforms recent supervised methods</strong>.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/SFCNN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Spherical Fractal Convolution Neural Networks for Point Cloud Recognition</papertitle>
              <br>
              <strong>Yongming Rao</strong>,  <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019
              <br>
              <a href="https://raoyongming.github.io/files/SFCNN.pdf">[PDF]</a> <a href="https://raoyongming.github.io/files/SFCNN_supplement.pdf">[Supplement]</a> 
              <br>
              <p> We designed Spherical Fractal Convolution Neural Networks (SFCNN) for rotation-invariant point cloud feature learning. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/RNR.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Runtime Network Routing for Efficient Image Classification</papertitle>
              <br>
              <strong>Yongming Rao</strong>,   <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="http://linji.me/"> Ji Lin </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2019
              <br>
              <a href="https://raoyongming.github.io/files/pami18.pdf">[PDF]</a> <a href="https://raoyongming.github.io/files/RNR.pytorch_release.zip">[Code]</a> <a href="https://papers.nips.cc/paper/6813-runtime-neural-pruning.pdf">[Conference Version (NeurIPS 2017)]</a>
              <br>
              <p> We propose a generic Runtime Network Routing (RNR) framework for efficient image classification, which selects an optimal path inside the network. Our method can be applied to off-the-shelf neural network structures and
                easily extended to various application scenarios. </p>
            </td>
          </tr>
        </tbody></table>

          <div id="click">
            <p align=right><a href="#Show-the-full-publication-list" style="padding:20px;" onclick="showStuff(this);">Full publication list</a></p>
           </div>
           <script>
             function showStuff(txt) {
               document.getElementById("full").style.display = "block";
               document.getElementById("click").style.display = "none";
              //  document.getElementById("selected").style.display = "none";
             }
             </script>

      <div id="full">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/NerfingMVS.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=NrRf7pUAAAAJ&hl=en"> Yi Wei </a>, <a href="http://b1ueber2y.me/academic.html"> Shaohui Liu</a>, <strong>Yongming Rao</strong>, Wang Zhao, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br>
              <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2109.01129">[arXiv]</a> <a href="https://github.com/weiyithu/NerfingMVS">[Code]</a> <a href="https://weiyithu.github.io/NerfingMVS">[Project page]</a> <a href="https://youtu.be/i-b5lPnYipA">[Video]</a>
              <br>
              <p> We present a new multi-view depth estimation method that utilizes both conventional SfM reconstruction and learning-based priors over the recently proposed neural radiance fields (NeRF).</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/CAL.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle> Counterfactual Attention Learning for
                Fine-Grained Visual Categorization and Re-identification </papertitle>
              <br>
              <strong>Yongming Rao</strong>*, <a href="https://chengy12.github.io/"> Guangyi Chen</a>*, 
                <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2108.08728">[arXiv]</a> <a href="https://github.com/raoyongming/CAL">[Code]</a> 
              <br>
              <p> We propose to learn the attention
                with counterfactual causality, which provides a tool to measure
                the attention quality and a powerful supervisory signal
                to guide the learning process. </p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/SPSR-PAMI.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Structure-Preserving Image Super-Resolution</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=Pt7JfRgAAAAJ&hl=en"> Cheng Ma </a>, <strong>Yongming Rao</strong>,  <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2109.12530">[arXiv]</a> <a href="https://github.com/Maclory/SPSR">[Code]</a> 
              <br>
              <p> We propose to learn a neural structure extractor unsupervisedly to extract structural patterns in images and use it to supervise SR models. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DIML.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle> Towards Interpretable Deep Metric Learning with Structural Matching
              </papertitle>
              <br>
              <a href="https://wl-zhao.github.io/"> Wenliang Zhao</a>*, <strong>Yongming Rao</strong>*, Ziyi Wang,
                <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2108.05889">[arXiv]</a> <a href="https://github.com/wl-zhao/DIML">[Code]</a> 
              <br>
              <p> We present a deep interpretable metric learning (DIML) that adopts a structural matching strategy to explicitly aligns the spatial embeddings by computing an optimal matching flow between feature maps of the two images. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/CoRe.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Group-aware Contrastive Regression for Action Quality Assessment</papertitle>
              <br>
              <a href="https://yuxumin.github.io">Xumin Yu</a>*, <strong>Yongming Rao</strong>*, <a href="https://wl-zhao.github.io/"> Wenliang Zhao</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>

              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2108.07797">[arXiv]</a> <a href="https://github.com/yuxumin/CoRe">[Code]</a>
            <br>
              <p>We propose a new contrastive regression (CoRe) framework to learn the relative scores by pair-wise comparison, which highlights the differences between videos and guides the models to learn the key hints for action quality assessment.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/PV_RAFT.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>PV-RAFT: Point-Voxel Correlation Fields for Scene Flow Estimation of Point Clouds</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=NrRf7pUAAAAJ&hl=en"> Yi Wei </a>*, Ziyi Wang*, <strong>Yongming Rao*</strong>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2012.00987">[arXiv]</a> <a href="https://github.com/weiyithu/PV-RAFT">[Code]</a>
              <br>
              <p></p>
              <p> We present point-voxel correlation fields for 3D scene flow estimation which migrates the high performance of RAFT and provides a solution to build structured all-pairs correlation fields for unstructured point clouds. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/WassClf.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Multi-Proxy Wasserstein Classifier for Image Classification</papertitle>
              <br>
              <a href="https://liubl1217.github.io/"> Benlin Liu </a>*, <strong>Yongming Rao*</strong>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, <a href="http://web.cs.ucla.edu/~chohsieh/"> Cho-Jui Hsieh </a>
              <br>
              <em>AAAI Conference on Artificial Intelligence  (<strong>AAAI</strong>)</em>, 2021
              <br>
              <a href="https://raoyongming.github.io/files/WassClf.pdf">[PDF]</a>
              <br>
              <p></p>
              <p>We present a new Multi-Proxy Wasserstein Classifier to imporve the image classification models by calculating a non-uniform matching
                flow between the elements in the feature map of a sample and multiple proxies of a class using optimal transport theory.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/coherence.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Temporal Coherence or Temporal Motion: Which is More Critical for Video-based Person Re-identification?</papertitle>
              <br>
              <a href="https://chengy12.github.io/"> Guangyi Chen </a>*, <strong>Yongming Rao*</strong>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020
              <br>
              <a href="https://raoyongming.github.io/files/temporal_coherence.pdf">[PDF]</a>
              <br>
              <p></p>
              <p>We show temporal coherence plays a more critical role than temporal motion for video-based person re-identification and develop a Adversarial Feature Augmentation (AFA) to highlight temporal coherence.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/metadistiller.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>MetaDistiller: Network Self-Boosting via Meta-Learned Top-Down Distillation</papertitle>
              <br>
              <a href="https://liubl1217.github.io/"> Benlin Liu </a>, <strong>Yongming Rao</strong>,  <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, <a href="http://web.cs.ucla.edu/~chohsieh/"> Cho-Jui Hsieh </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2008.12094">[arXiv]</a>
              <br>
              <p></p>
              <p>We boost the performance of CNNs by learning soft targets for shallow layers via meta-learning.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/SPSR.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Structure-Preserving Super Resolution with Gradient Guidance</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=Pt7JfRgAAAAJ&hl=en"> Cheng Ma </a>, <strong>Yongming Rao</strong>, Yean Cheng, Ce Chen,  <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2003.13081">[arXiv]</a> <a href="https://github.com/Maclory/SPSR">[Code]</a> 
              <br>
              <p> We propose to leverage gradient information as an extra supervision signal to restore structures while generating natural SR images. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/faceSR.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Deep Face Super-Resolution with Iterative Collaboration between Attentive Recovery and Landmark Estimation</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=Pt7JfRgAAAAJ&hl=en"> Cheng Ma </a>,  <a href="https://zhenyujiang.me/"> Zhenyu Jiang </a>, <strong>Yongming Rao</strong>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020
              <br>
              <a href="https://arxiv.org/abs/2003.13063">[arXiv]</a> <a href="https://github.com/Maclory/Deep-Iterative-Collaboration">[Code]</a> 
              <br>
              <p> We propose a deep face super-resolution method with iterative collaboration between two recurrent networks which focus on facial image recovery and landmark estimation respectively </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/COIN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>COIN: A Large-scale Dataset for Comprehensive Instructional Video Analysis</papertitle>
              <br>
              <a href="https://andytang15.github.io/"> Yansong Tang </a>, Dajun Ding, <strong>Yongming Rao</strong>, Yu Zheng, Danyang Zhang, Lili Zhao, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2019
              <br>
              <a href="https://arxiv.org/abs/1903.02874">[arXiv]</a>  <a href="https://coin-dataset.github.io/">[Project Page]</a>  <a href="https://github.com/coin-dataset/annotation-tool">[Annotation Tool]</a> 
              <br>
              <p> COIN is the largest and most comprehensive instructional video analysis dataset with rich annotations. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%"  src='images/IJCV_DAN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Discriminative Aggregation Network for Video-based Face Recognition and Person Re-identification</papertitle>
              <br>
              <strong>Yongming Rao</strong>,  <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>International Journal of Computer Vision (<strong>IJCV</strong>, IF: 6.07)</em>, 2019
              <br>
              <a href="https://raoyongming.github.io/files/IJCV_DAN.pdf">[PDF]</a> <a href="https://raoyongming.github.io/files/DAN.py">[Code]</a>
              <br>
              <p> We propose a discriminative aggregation network (DAN) method for video-based face recognition and person re-identification, which aims to integrate
                information from video frames for feature representation effectively and efficiently. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/GODet.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Globally Optimized Object Detector via Policy Gradient</papertitle>
              <br>
              <strong>Yongming Rao</strong>, <a href="http://dahua.site/"> Dahua Lin </a>,  <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2018
              <br>
              <font color="red"><strong>Spotlight Presentation</strong></font>
              <br>
              <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/2657.pdf">[PDF]</a> <a href="https://raoyongming.github.io/files/cvpr18_supplement.pdf">[Supplement]</a>
              <br>
              <p> We propose a simple yet effective method to learn globally optimized detector for object detection by directly optimizing mAP using the REINFORCE algorithm. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/RNP.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Runtime Neural Pruning</papertitle>
              <br>
              <a href="http://linji.me/"> Ji Lin </a>*, <strong>Yongming Rao*</strong>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2017
              <br>
              <a href="https://papers.nips.cc/paper/6813-runtime-neural-pruning.pdf">[PDF]</a> <a href="https://raoyongming.github.io/files/RNR.pytorch_release.zip">[Code]</a>
              <br>
              <p> We propose a Runtime Neural Pruning (RNP) framework which prunes the deep neural network dynamically at the runtime. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/DAN.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Learning Discriminative Aggregation Network for Video-Based Face Recognition</papertitle>
              <br>
              <strong>Yongming Rao</strong>, <a href="http://linji.me/"> Ji Lin </a>,  <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2017
              <br>
              <font color="red"><strong>Spotlight Presentation</strong></font>
              <br>
              <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Rao_Learning_Discriminative_Aggregation_ICCV_2017_paper.pdf">[PDF]</a>  <a href="https://raoyongming.github.io/files/DAN.py">[Code]</a> <a href="https://raoyongming.github.io/files/dan_supplement.pdf">[Supplement]</a>
              <br>
              <p> We propose a discriminative aggregation network (DAN) method for video face recognition, which aims to integrate information from video frames effectively and efficiently. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ADRL.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Attention-aware Deep Reinforcement Learning for Video Face Recognition</papertitle>
              <br>
              <strong>Yongming Rao</strong>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2017
              <br>
              <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Rao_Attention-Aware_Deep_Reinforcement_ICCV_2017_paper.pdf">[PDF]</a>
              <br>
              <p> We propose an attention-aware deep reinforcement learning (ADRL) method for video face recognition,
                which aims to discard the misleading and confounding frames and find the focuses of attentions in face videos for person recognition. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/VTree.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>V-tree: Efficient KNN Search on Moving Objects with Road-Network Constraints</papertitle>
              <br>
              Bilong Shen, Ying Zhao, Guoliang Li, Weimin Zheng, Yue Qin, Bo Yuan, <strong>Yongming Rao</strong>
              <br>
              <em>IEEE International Conference on Data Engineering (<strong>ICDE</strong>)</em>, 2017
              <br>
              <a href="https://raoyongming.github.io/files/vtree.pdf">[PDF]</a>
              <br>
              <p> We propose a new tree structure for moving objects kNN search with road-network constraints, which can be used in many real-world applications like taxi search.  </p>
            </td>
          </tr></tbody></table>
      </div>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> 1st place in the MVP Point Cloud Completion Challenge (ICCV 2021 Workshop)</li>
                <li style="margin: 5px;"> Baidu Top 100 Chinese Rising Stars in AI <a href="https://mp.weixin.qq.com/s/v7ITiZXOJiDUbPlRlcqQRA">(百度AI华人新星百强榜)</a></li>
                <li style="margin: 5px;"> CVPR 2021 Outstanding Reviewer</li>
                <li style="margin: 5px;"> ECCV 2020 Outstanding Reviewer</li>
                <li style="margin: 5px;"> 2nd place in Semi-Supervised Recognition Challenge at FGVC7 (CVPR 2020 Workshop)</li>
                <li style="margin: 5px;"> 2019 CCF-CV Academic Emerging Award (CCF-CV 学术新锐奖)</li>
                <li style="margin: 5px;"> 2019 Chinese National Scholarship </li>
                <li style="margin: 5px;"> ICME 2019 Best Reviewer Award </li>
                <li style="margin: 5px;"> 2017 Sensetime Undergraduate Scholarship </li>
                <li style="margin: 5px;"> 1st place in 17th Electronic Design Contest of Tsinghua University </li>
                <li style="margin: 5px;"> 1st place in Momenta Lane Detection Challenge </li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Co-organizer:</b> Tutorial on Deep Reinforcement Learning for Computer Vision at CVPR 2019 <a href="http://ivg.au.tsinghua.edu.cn/DRLCV/"> [website]</a>
              </li>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer / PC Member:</b> CVPR 2018-2022, ICCV 2019-2021, ECCV 2020-2022, NeurIPS 2019-2022, ICML 2019-2022, ICLR 2021-2023, SIGGRAPH Asia 2022, AAAI 2020-2022, WACV 2020-2022, ICME 2019-2022, 
              </li>
              <li style="margin: 5px;"> 
                <b>Senior PC Member:</b> IJCAI 2021
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b> T-PAMI, IJCV, T-NNLS, T-IP, T-MM, Pattern Recognition
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<!-- <p><center>
	  <div id="clustrmaps-widget" style="width:5%">
	  <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=yp3s8rdiQW_pbzmBOzWDx2Fv6afIlEpV-k1EZiYIkEY"></script>
	  </div>        
	  <br>
	    &copy; Yongming Rao | Last updated: Oct 11, 2022
</center></p> -->
</body>

</html>
