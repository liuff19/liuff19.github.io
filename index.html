<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Fangfu Liu</title>
  
  <meta name="author" content="Fangfu Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Fangfu Liu | 刘芳甫</name>
              </p>
              <p> 
                I am a first-year Ph.D student in the Department of <a href="https://www.ee.tsinghua.edu.cn/en/"> Electronic Engineering </a> at <a href="https://www.tsinghua.edu.cn/en/"> Tsinghua University </a>, advised by Prof. <a href="https://duanyueqi.github.io/">Yueqi Duan</a>. In 2023, I obtained my B.Eng. in the Department of Electronic Engineering, Tsinghua University. 
              </p>
              <p>
                My research interest lies in the <b>Machine Learning</b> and <b>3D Computer Vision</b>. I aim to build reliable models with a focus on their generalization ability (to unseen data or different domains), robustness (to data noise and bias), and efficiency.
              </p>
              <p style="text-align:center">
                <a href="mailto:fangfu19@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://liuff19.github.io/">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=b-4FUVsAAAAJ&hl=zh-CN&oi=ao"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/liuff19"> Github </a> &nbsp/&nbsp
		<a href="https://x.com/fangfu0830"> Twitter </a>
              </p>
            </td>
            <td style="padding:3%;width:40%;max-width:40%">
              <img style="width:70%;max-width:70%" alt="profile photo" src="images/graduate.jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
	      <li style="margin: 5px;" >
              <b>2024-07:</b> Three papers on 3D AIGC are accepted by <a href="https://eccv.ecva.net/">ECCV 2024</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-02:</b> One paper on 3D AIGC is accepted by <a href="https://cvpr.thecvf.com/">CVPR 2024</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-05:</b> One paper on Structure Learning is accepted by <a href="https://kdd.org/kdd2023/">KDD 2023</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-02:</b> One paper on NeRF is accepted by <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-01:</b> One paper on Causal Discovery is accepted by <a href="https://iclr.cc/">ICLR 2023</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Publications</heading></p>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Physics3D.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Physics3D: Learning Physical Properties of 3D Gaussians via Video Diffusion</papertitle>
              <br>
              <strong>Fangfu Liu*</strong>, 
              Hanyang Wang*
              <a href="https://scholar.google.com/citations?user=i4kyLbwAAAAJ"> Shunyu Yao </a>,
              Shengjun Zhang,
              <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ"> Jie Zhou</a>,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>
              <br>
              <em>Arxiv, 2024</em>
              <br>
              <a href="https://arxiv.org/abs/2406.04338">[arXiv]</a>
              <a href="https://github.com/liuff19/Physics3D">[Code]</a>
              <a href="https://liuff19.github.io/Physics3D">[Project Page]</a> 
              <br>
              <p> In this paper, we propose Physics3D, a novel method for learning various physical properties of 3D objects through a video diffusion model. Our approach involves designing a highly generalizable physical simulation system based on a viscoelastic material model, which enables us to simulate a wide range of materials with high-fidelity capabilities. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Unique3d.jpg" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=VTU0gysAAAAJ&hl=zh-CN&oi=ao"> Kailu Wu </a>, 
              <strong>Fangfu Liu</strong>, 
              Zhihan Cai, 
              Runjie Yan, 
              Hanyang Wang, 
              Yating Hu,
              <br>
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
              <a href="https://group.iiis.tsinghua.edu.cn/~maks/"> Kaisheng Ma </a>
              <br>
              <em>Arxiv, 2024</em>
              <br>
              <a href="https://arxiv.org/abs/2405.20343">[arXiv]</a>
              <a href="https://github.com/AiuniAI/Unique3D">[Code]</a>
              <a href="https://wukailu.github.io/Unique3D/">[Project Page]</a> 
              <br>
              <p> In this work, we introduce Unique3D, a novel image-to-3D framework for efficiently generating high-quality 3D meshes from single-view images, featuring state-of-the-art generation fidelity and strong generalizability. Unique3D can generate a high-fidelity textured mesh from a single orthogonal RGB image of any object in under 30 seconds.  </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DreamReward.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DreamReward: Text-to-3D Generation with Human Preference</papertitle>
              <br>
              <a href="https://jamesyjl.github.io/"> Junliang Ye* </a>, 
              <strong>Fangfu Liu*</strong>, 
              Qixiu Li,
              <a href="https://thuwzy.github.io/"> Zhengyi Wang </a>,
              <a href="https://yikaiw.github.io/"> Yikai Wang </a>,
              <br>
              Xinzhou Wang,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
              <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml"> Jun Zhu </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2403.14613">[arXiv]</a>
              <a href="https://github.com/liuff19/DreamReward">[Code]</a>
              <a href="https://jamesyjl.github.io/DreamReward/">[Project Page]</a> 
              <br>
              <p> In this work, We propose the first general-purpose human preference reward model for text-to-3D generation, named Reward3D. Then we further introduce a novel text-to-3D framework, coined DreamReward, which greatly boosts high-text alignment and high-quality 3D generation through human preference feedback. </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/makeyour3d.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Make-Your-3D: Fast and Consistent Subject-Driven 3D Content Generation</papertitle>
              <br>
              <strong>Fangfu Liu</strong>, 
              Hanyang Wang, 
              Weiliang Chen,
              Haowen Sun,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2403.09625">[arXiv]</a>
              <a href="https://github.com/liuff19/Make-Your-3D">[Code]</a>
              <a href="https://liuff19.github.io/Make-Your-3D/">[Project Page]</a> 
              <br>
              <p> We introduce a novel 3D customization method, dubbed Make-Your-3D that can personalize high-fidelity and consistent 3D content from only a single image of a subject with text description within 5 
                  minutes. </p>
            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Sherpa.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Sherpa3D: Boosting High-Fidelity Text-to-3D Generation via Coarse 3D Prior</papertitle>
              <br>
              <strong>Fangfu Liu</strong>, 
              Diankun Wu, 
              <a href="https://weiyithu.github.io/"> Yi Wei </a>, 
              <a href="https://raoyongming.github.io/"> Yongming Rao </a>,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2312.06655">[arXiv]</a>
              <a href="https://github.com/liuff19/Sherpa3D">[Code]</a>
              <a href="https://liuff19.github.io/Sherpa3D/">[Project Page]</a> 
              <br>
              <p> We propose Sherpa3D, a new text-to-3D framework that achieves high-fidelity, generalizability, and geometric consistency simultaneously. Extensive experiments show the superiority of our Sherpa3D over the state-of-the-art text-to-3D methods in terms of quality and 3D consistency.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/CASPER.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Discovering Dynamic Causal Space for DAG Structure Learning</papertitle>
              <br>
              <strong>Fangfu Liu</strong>, 
              Wenchang Ma, 
              <a href="https://anzhang314.github.io/index.html"> An Zhang </a>, 
              <a href="https://xiangwang1223.github.io/"> Xiang Wang </a>,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
              <a href="https://scholar.google.com/citations?user=Z9DWCBEAAAAJ&hl=en&oi=ao"> Tat-Seng Chua </a>
              <br>
              <em>ACM SIGKDD Conference on Knowledge Discovery and Data Mining (<strong>KDD</strong>)</em>, 2023
              <br>
              <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2306.02822">[arXiv]</a>
              <a href="https://arxiv.org/abs/2306.02822">[Code]</a>
              <a href="https://arxiv.org/abs/2306.02822">[Project Page]</a> 
              <br>
              <p> we propose a dynamic causal space for DAG structure learning, coined CASPER, that integrates the graph structure into the score function as a new measure in the causal space to faithfully reflect the causal distance between estimated and groundtruth DAG.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/S-Ray.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Semantic Ray: Learning a Generalizable Semantic Field with Cross-Reprojection Attention</papertitle>
              <br>
              <strong>Fangfu Liu</strong>, 
              Chubin Zhang, 
              <a href="https://yzheng97.github.io/"> Yu Zheng</a>, 
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.13014">[arXiv]</a>
              <a href="https://github.com/liuff19/Semantic-Ray">[Code]</a>
              <a href="https://liuff19.github.io/S-Ray/">[Project Page]</a> 
              <br>
              <p> We propose a neural semantic representation called Semantic-Ray (S-Ray) to build a generalizable semantic field, which is able to learn from multiple scenes and directly infer semantics on novel viewpoints across novel scenes.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/rescore.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Boosting Differentiable Causal Discovery via Adaptive Sample Reweighting</papertitle>
              <br>
              <a href="https://anzhang314.github.io/index.html"> An Zhang</a>,
              <strong>Fangfu Liu</strong>, 
              Wenchang Ma,
              Zhibo Cai,
              <a href="https://xiangwang1223.github.io/"> Xiang Wang </a>,
              <a href="https://scholar.google.com/citations?user=Z9DWCBEAAAAJ&hl=en&oi=ao"> Tat-Seng Chua </a>
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.03187">[arXiv]</a>
              <a href="https://github.com/anzhang314/ReScore">[Code]</a>
              <a href="https://arxiv.org/abs/2303.03187">[Project Page]</a> 
              <br>
              <p> We propose ReScore, a simple-yet-effective model-agnostic optimzation framework that simultaneously eliminates spurious edge learning and generalizes to heterogeneous data by utilizing learnable adaptive weights.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/VL-grasp.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>VL-Grasp: a 6-Dof Interactive Grasp Policy for Language-Oriented Objects in Cluttered Indoor Scenes</papertitle>
              <br>
              Yuhao Lu,
              Yixuan Fan,
              Beixing Deng,
              <strong>Fangfu Liu</strong>, 
              Yali Li,
              <a href="https://scholar.google.com/citations?user=RgzLZZsAAAAJ&hl=en&oi=ao"> Shengjin Wang </a>
              <br>
              <em>International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2308.00640">[arXiv]</a>
              <a href="https://github.com/luyh20/VL-Grasp">[Code]</a>
              <a href="https://arxiv.org/abs/2308.00640">[Project Page]</a> 
              <br>
              <p> The VL-Grasp is an interactive grasp policy combined with visual grounding and 6-dof grasp pose detection tasks. The robot can adapt to various observation views and more diverse indoor scenes to grasp the target according to a human's language command by applying the VL-Grasp. Meanwhile, we build a new visual grounding dataset specially designed for the robot interaction grasp task, called RoboRefIt.</p>
            </td>
          </tr>



        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> <b>National Scholarship (Top 1 in 260+ in the 2019-2020 academic year)</b></li>
                <li style="margin: 5px;"> Tsinghua University Comprehensive Excellent Award <b>twice (Top 5% in 260+, 2020&2021)</b></li>
                <li style="margin: 5px;"> Tsinghua Science and Technology Innovation Excellence Award (2022)</li>
                <li style="margin: 5px;"> Four star Bauhinia volunteer of Tsinghua University (Volunteer hours up to 150, 2021)</li>
                <li style="margin: 5px;"> Advanced individual award of Tsinghua University (2019)</li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                Review for <b>PRCV 2024</b>, <b>TCSVT</b>, <b>NeurIPS 2022</b>, <b>IROS 2023</b>, <b>ACMMM 2024</b>, <b>NeurIPS 2024</b>.
              </li>
          </td>
        </tr>
      </tbody></table>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
      <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=B_hoqUcZkAVteexiuKv_tIvNw9enA1g2tIC3ypxXP2E"></script>
	  <br>
	    &copy; Fangfu Liu | Last updated: 02 July, 2024
</center></p>
</body>

</html>
