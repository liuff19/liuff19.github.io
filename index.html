<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Fangfu Liu</title>
  
  <meta name="author" content="Fangfu Liu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Fangfu Liu | 刘芳甫</name>
              </p>
              <p> 
                I am a one year Ph.D student in the Department of <a href="https://www.ee.tsinghua.edu.cn/en/"> Electronic Engineering </a> at <a href="https://www.tsinghua.edu.cn/en/"> Tsinghua University </a>, advised by Prof. <a href="https://duanyueqi.github.io/">Yueqi Duan</a>. In 2023, I obtained my B.Eng. in the Department of Electronic Engineering, Tsinghua University. 
                
                My research interest lies at the <b>causality</b>, <b>machine learning</b> and <b>3D computer vision</b>. I aim to build reliable models with a focus on their generalization ability (to unseen data or different domains), robustness (to data noise and bias) and explainability.
              </p>
              <p style="text-align:center">
                <a href="mailto:fangfu19@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://liuff19.github.io/">CV</a> &nbsp/&nbsp
                <a href="liuff19.github.io"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/liuff19"> Github </a>
              </p>
            </td>
            <td style="padding:3%;width:40%;max-width:40%">
              <img style="width:70%;max-width:70%" alt="profile photo" src="images/graduate.jpg" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2023-05:</b> One paper on Structure Learning is accepted by <a href="https://kdd.org/kdd2023/">KDD 2023</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-02:</b> One paper on NeRF is accepted by <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-01:</b> One paper on Causal Discovery is accepted by <a href="https://iclr.cc/">ICLR 2023</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Publications</heading></p>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/CASPER.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Discovering Dynamic Causal Space for DAG Structure Learning</papertitle>
              <br>
              <strong>Fangfu Liu</strong>, 
              Wenchang Ma, 
              <a href="https://anzhang314.github.io/index.html"> An Zhang </a>, 
              <a href="https://xiangwang1223.github.io/"> Xiang Wang </a>,
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>,
              <a href="https://scholar.google.com/citations?user=Z9DWCBEAAAAJ&hl=en&oi=ao"> Tat-Seng Chua </a>
              <br>
              <em>ACM SIGKDD Conference on Knowledge Discovery and Data Mining (<strong>KDD</strong>)</em>, 2023
              <br>
              <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2306.02822">[arXiv]</a>
              <a href="https://arxiv.org/abs/2306.02822">[Code]</a>
              <a href="https://arxiv.org/abs/2306.02822">[Project Page]</a> 
              <br>
              <p> we propose a dynamic causal space for DAG structure learning, coined CASPER, that integrates the graph structure into the score function as a new measure in the causal space to faithfully reflect the causal distance between estimated and groundtruth DAG.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/S-Ray.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Semantic Ray: Learning a Generalizable Semantic Field with Cross-Reprojection Attention</papertitle>
              <br>
              <strong>Fangfu Liu</strong>, 
              Chubin Zhang, 
              <a href="https://yzheng97.github.io/"> Yu Zheng</a>, 
              <a href="https://duanyueqi.github.io/"> Yueqi Duan </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.13014">[arXiv]</a>
              <a href="https://github.com/liuff19/Semantic-Ray">[Code]</a>
              <a href="https://liuff19.github.io/S-Ray/">[Project Page]</a> 
              <br>
              <p> We propose a neural semantic representation called Semantic-Ray (S-Ray) to build a generalizable semantic field, which is able to learn from multiple scenes and directly infer semantics on novel viewpoints across novel scenes.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/rescore.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Boosting Differentiable Causal Discovery via Adaptive Sample Reweighting</papertitle>
              <br>
              <a href="https://anzhang314.github.io/index.html"> An Zhang</a>,
              <strong>Fangfu Liu</strong>, 
              Wenchang Ma,
              Zhibo Cai,
              <a href="https://xiangwang1223.github.io/"> Xiang Wang </a>,
              <a href="https://scholar.google.com/citations?user=Z9DWCBEAAAAJ&hl=en&oi=ao"> Tat-Seng Chua </a>
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.03187">[arXiv]</a>
              <a href="https://github.com/anzhang314/ReScore">[Code]</a>
              <a href="https://arxiv.org/abs/2303.03187">[Project Page]</a> 
              <br>
              <p> We propose ReScore, a simple-yet-effective model-agnostic optimzation framework that simultaneously eliminates spurious edge learning and generalizes to heterogeneous data by utilizing learnable adaptive weights.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/VL-grasp.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>VL-Grasp: a 6-Dof Interactive Grasp Policy for Language-Oriented Objects in Cluttered Indoor Scenes</papertitle>
              <br>
              Yuhao Lu,
              Yixuan Fan,
              Beixing Deng,
              <strong>Fangfu Liu</strong>, 
              Yali Li,
              <a href="https://scholar.google.com/citations?user=RgzLZZsAAAAJ&hl=en&oi=ao"> Shengjin Wang </a>
              <br>
              <em>International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2308.00640">[arXiv]</a>
              <a href="https://github.com/luyh20/VL-Grasp">[Code]</a>
              <a href="https://arxiv.org/abs/2308.00640">[Project Page]</a> 
              <br>
              <p> The VL-Grasp is an interactive grasp policy combined with visual grounding and 6-dof grasp pose detection tasks. The robot can adapt to various observation views and more diverse indoor scenes to grasp the target according to a human's language command by applying the VL-Grasp. Meanwhile, we build a new visual grounding dataset specially designed for the robot interaction grasp task, called RoboRefIt.</p>
            </td>
          </tr>



        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> <b>National Scholarship (Top 1 in 260+ in the 2019-2020 academic year)</b></li>
                <li style="margin: 5px;"> Tsinghua University Comprehensive Excellent Award <b>twice (Top 5% in 260+, 2020&2021)</b></li>
                <li style="margin: 5px;"> Tsinghua Science and Technology Innovation Excellence Award (2022)</li>
                <li style="margin: 5px;"> Four star Bauhinia volunteer of Tsinghua University (Volunteer hours up to 150, 2021)</li>
                <li style="margin: 5px;"> Advanced individual award of Tsinghua University (2019)</li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                Review for <b>NeurIPS 2022</b>, <b>IROS 2023</b>.
              </li>
          </td>
        </tr>
      </tbody></table>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
      <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=B_hoqUcZkAVteexiuKv_tIvNw9enA1g2tIC3ypxXP2E"></script>
	  <br>
	    &copy; Fangfu Liu | Last updated: Sept 1, 2023
</center></p>
</body>

</html>
