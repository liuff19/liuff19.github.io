<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="Semantic Ray: Learning a Generalizable Semantic Field with Cross-Reprojeciton Attention">
    <meta name="author" content="Fangfu Liu,
                                Chubin Zhang,
                                Yu Zheng,
                                Yueqi Duan">

    <title>Semantic Ray: Learning a Generalizable Semantic Field with Cross-Reprojeciton Attention</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>Semantic-Ray: Learning a Generalizable Semantic Field with<br>Cross-Reprojection Attention</h2>
    <h3>CVPR 2023</h3>
<!--            <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>-->
    <hr>
    <p>
        <span style="white-space: nowrap;">
        Fangfu Liu<sup>1</sup>&nbsp;&nbsp;
        Chubin Zhang<sup>2</sup>&nbsp;&nbsp;
        Yu Zheng<sup>2</sup>&nbsp;&nbsp;
        Yueqi Duan<sup>1&#8224;</sup>
        </span>
        <br><br>
        <span style="font-size: smaller;">
        <sup>1</sup>Department of Electronic Engineering, Tsinghua University &nbsp;&nbsp;&nbsp;&nbsp;
        <sup>2</sup>Department of Automation, Tsinghua University
        </span>
        <br><br>
        <span style="font-size: smaller;">
        <a href="https://arxiv.org/abs" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/file.png" alt="paper" style="vertical-align: middle;">
            &nbsp;Paper (arXiv)
        </a>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://github.com/your_github_repo" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/github.png" alt="code" style="vertical-align: middle;">
            &nbsp;Code (GitHub)
        </a>
        </span>
    </p>

    <!-- <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2006.09661">Paper</a>
        <a class="btn btn-primary" href="https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb">Colab Notebook</a>
        <a class="btn btn-primary" href="https://dcato98.github.io/playground/#activation=sine">Tensorflow Playground</a>
        <a class="btn btn-primary" href="https://github.com/vsitzmann/siren">Code</a>
        <a class="btn btn-primary" href="https://drive.google.com/drive/u/1/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K">Data</a>
    </div> -->
</div>

<div class="container">
    <div class="section">
        <h2>Abstract</h2>
        <hr>
        <p>
            In this paper, we aim to learn a semantic radiance field from multiple scenes that is accurate, efficient and 
            generalizable. While most existing NeRFs target at the tasks of neural scene rendering, image synthesis and multi- 
            view reconstruction, there are a few attempts such as Semantic NeRF that explore to learn high-level semantic 
            understanding with the NeRF structure. However, Semantic-NeRF simultaneously learns color and semantic label from a 
            single ray with multiple heads, where single rays fail to provide rich semantic information. As a result, Semantic 
            NeRF relies on positional encoding and needs to train one independent model for each scene. To address this, we 
            propose Semantic Ray (S-Ray) to fully exploit semantic information along the ray direction from its multi-view 
            reprojections. As directly performing dense attention over multi-view reprojected rays would suffer from heavy 
            computational cost, we design a Cross-Reprojection Attention model with consecutive radial and cross-view sparse 
            attentions, which decomposes contextual information along reprojected rays and cross multiple views and then collects 
            dense connections by stacking the modules. Experiments show that our S-Ray is able to learn from multiple scenes, and 
            it presents strong generalization ability to adapt to unseen scenes.
        </p>
    </div>

    <div class="section">
        <h2>Generalizable Semantic Field</h2>
        <hr>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/teaser.png" alt="teaser" width="100%">
            </div>
        </div>
        <p>
            <b>Top: </b>Comparisons between Semantic-NeRF and our method Semantic-Ray. Semantic-NeRF (S-NeRF for short) needs to train one independent model for each scene, 
            while our Semantic-Ray (S-Ray for short) is able to simultaneously train on multiple scenes and generalize to unseen scenes. <b>Bottom: </b>Experimental comparisons   
            between S-Ray and S-NeRF on generalization ability. We observe that our network S-Ray can effectively <b>fast generalize</b> across diverse unseen scenes while S-NeRF 
            fails in a new scene. Moreover, our result can be improved by fine-tuning on more images <b>for only 10 min (2k iterations)</b>, which achieves comparable quality 
            with the Semantic-NeRF's result for 100k iterations per-scene optimization.
        </p>
        <!-- <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/image_convergence_15s_label.mp4" type="video/mp4">
                </video>
            </div> 
        </div>
        <div class="row align-items-center">
			<div class="col justify-content-center text-center">
                <video width="40%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/psnr_image_convergence_15s.mp4" type="video/mp4">
                </video>
            </div> 
        </div> -->
    </div>

    <div class="section">
        <h2>Method</h2>
        <hr>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/pipeline.png" alt="pipeline" width="100%">
            </div>
        </div>
        <p>
            Pipeline of semantic rendering with S-Ray. Given input views and a query ray, we first reproject the ray to each input view and apply a CNN-based module to 
            extract contextual features to build an initial 3D contextual space. Then, we apply the cross-reprojection attention module to learn dense semantic connections 
            and build a refined contextual space. For semantic ray rendering, we adopt the semantic-aware weight net to learn the significance of each view to construct 
            our semantic ray from refined contextual space. Finally, we leverage the geometry-aware net to get the density and render the semantics along the query ray.
        </p>
    </div>

    <div class="section">
        <h2>Cross-Reprojection Attention Module</h2>
        <hr>
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/CRA.png" alt="CRA_module" width="100%">
            </div>
        </div>
        <div style="display: flex; justify-content: center; align-items: center; flex-direction: column;">
            <img src="https://i.imgur.com/Xt2drin.png" alt="Pipeline of Cross-Reprojection Attention" width="800">
            <p style="font-size: 16px; margin-top: 10px; text-align: center;">
                <b>Pipeline of Cross-Reprojection Attention.</b> Given the initial 3D contextual space, we first decompose &#119980; along the 
                radial direction (i.e., each intra-view). Then, we apply the intra-view radial attention module to each &#119975;<sub>i</sub> (i=1, &#8230;, m) to learn the 
                ray-aligned contextual feature from each source view and build the &#119980;&#8242;. We further decompose the &#119980;&#8242; cross multiple views and 
                employ the cross-view sparse attention module to each &#119975;&#8242;<sub>r*(i)</sub>, thus capturing sparse contextual patterns with their respective 
                significance to semantics. After the two consecutive attention modules, we fuse the decomposed contextual information with the final refined 3D contextual 
                space &#119980;&#8242;&#8242;, which models dense semantic collections around the ray. (Best viewed in color)
            </p>
          </div>
        <!-- <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="50%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/cat_comparison_label.mp4" type="video/mp4">
                </video>
            </div>
            <div class="col justify-content-center text-center">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/bikes_comparison_label.mp4" type="video/mp4">
                </video>
            </div>
        </div> -->
    </div>

    <div class="section">
        <h2>Solving the Poisson Equation</h2>
        <hr>
        <p>
            By supervising only the derivatives of Siren, we can solve <a href="https://en.wikipedia.org/wiki/Poisson%27s_equation">Poisson's equation</a>.
            Siren is again the only architecture that fits image, gradient, and laplacian domains accurately and swiftly.
        </p>
         <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/poisson_convergence_15s_label.mp4" type="video/mp4">
                </video>
            </div> 
        </div>
        <div class="row align-items-center">
			<div class="col justify-content-center text-center">
                <video width="40%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/psnr_poisson_convergence_15s.mp4" type="video/mp4">
                </video>
            </div> 
        </div>
    </div>

    <div class="section">
        <h2>Representing shapes by solving the Eikonal equation<br>
            Interactive 3D SDF Viewer - Use Your Mouse to Navigate the Scenes</h2>
        <hr>
        <p>
            We can recover an SDF from a pointcloud and surface normals by solving the <a
                href="https://www.google.com/search?client=ubuntu&channel=fs&q=eikonal+equation&ie=utf-8&oe=utf-8">Eikonal
            equation</a>,
            a first-order boundary value problem. SIREN can recover a room-scale scene given only its pointcloud
            and surface normals, accurately reproducing fine detail, in less than an hour of training.
            In contrast to recent work on combining voxel grids with neural implicit representations,
            this stores the full scene in the weights of a single, 5-layer neural network, with no 2D or 3D
            convolutions, and orders of magnitude fewer parameters. Zoom in to compare fine detail!
            <b>Note that these SDFs are not supervised with ground-truth SDF / occupancy values, but rather, are the
            result of solving the above Eikonal boundary value problem. This is a significantly harder task,
            which requires supervision in the gradient domain (see paper). As a result, architectures whose gradients
            are not well-behaved perform worse than SIREN.</b>
        </p>
        <div class="container">
            <div class="row align-items-center">
                <div class="col-md-6 padding-0 canvas-row">
                    <h4>Room - Siren</h4>
                    <model-viewer
                            alt="Room Siren"
                            src="img/room_siren.glb"
                            style="width: 100%; height:300px; background-color: #404040"
                            exposure=".8"
                            camera-orbit="0deg 75deg 105%"
                            auto-rotate
                            camera-controls>
                    </model-viewer>
                </div>
                <div class="col-md-6 padding-0 canvas-row">
                    <h4>Room - ReLU</h4>
                    <model-viewer
                            alt="Room ReLU"
                            src="img/room_relu.glb"
                            style="width: 100%; height: 300px; background-color: #404040"
                            exposure=".8"
                            auto-rotate
                            camera-controls>
                    </model-viewer>
                    <!--                            poster="https://uploads-ssl.webflow.com/51e0d73d83d06baa7a00000f/5e7e6db2d75a9b467eee4111_legomesh_cover.png"-->
                </div>
            </div>
            <div class="row align-items-center">
                <div class="col-md-4 padding-0 canvas-row">
                    <h4>Statue - Siren</h4>
                    <model-viewer
                            alt="Statue Siren"
                            src="img/statue_siren.glb"
                            style="width: 100%; height: 600px; background-color: #404040"
                            exposure=".8"
                            camera-orbit="0deg 75deg 20%"
                            auto-rotate
                            camera-controls>
                    </model-viewer>
                </div>
                <div class="col-md-4 padding-0 canvas-row">
                    <h4>Statue - ReLU Pos. Enc.</h4>
                    <model-viewer
                            alt="Statue Positional Encoding"
                            src="img/statue_relu_pe.glb"
                            style="width: 100%; height: 600px; background-color: #404040"
                            exposure=".8"
                            camera-orbit="0deg 75deg 20%"
                            auto-rotate
                            camera-controls>
                    </model-viewer>
                </div>
                <div class="col-md-4 padding-0 canvas-row">
                    <h4>Statue - ReLU</h4>
                    <model-viewer
                            alt="Statue ReLU"
                            src="img/statue_relu.glb"
                            style="width: 100%; height: 600px; background-color: #404040"
                            exposure=".8"
                            camera-orbit="0deg 75deg 20%"
                            auto-rotate
                            camera-controls>
                    </model-viewer>
                </div>
            </div>
        </div>

        <!-- Loads <model-viewer> for modern browsers: -->
        <script type="module"
                src="https://unpkg.com/@google/model-viewer/dist/model-viewer.js">
        </script>
    </div>

    <div class="section">
        <h2>Solving the Helmholtz equation</h2>
        <hr>
        <p>
            Here, we use Siren to solve the <a href="https://en.wikipedia.org/wiki/Helmholtz_equation">inhomogeneous Helmholtz equation</a>.
            ReLU- and Tanh-based architectures fail entirely to converge to a solution.
        </p>
        <div class="gif">
            <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                <source src="img/helmholtz_convergence_video_pad_label.mp4" type="video/mp4">
            </video>
        </div>
    </div>

    <div class="section">
        <h2>Solving the wave equation</h2>
        <hr>
        <p>
            In the time domain, Siren succeeds to solve the wave equation, while a Tanh-based architecture fails to discover the
            correct solution.
        </p>
        <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="" class="gif">
            <source src="img/wave_combined_pad_label.mp4" type="video/mp4">
        </video>
    </div>

    <div class="section">
        <h2>Related Projects</h2>
        <hr>
        <p>
            Check out our related projects on the topic of implicit neural representations! <br>
        </p>
        <div class='row vspace-top'>
            <div class="col-sm-3">
                <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="img/metasdf_steps_comp.mp4" type="video/mp4">
                </video>
            </div>

            <div class="col">
                <div class='paper-title'>
                    <a href="http://vsitzmann.github.io/metasdf/">MetaSDF: Meta-learning Signed Distance Functions</a>
                </div>
                <div>
                    We identify a key relationship between generalization across implicit neural representations and meta-
                    learning, and propose to leverage gradient-based meta-learning for learning priors over deep signed distance
                    functions. This allows us to reconstruct SDFs an order of magnitude faster than the auto-decoder framework,
                    with no loss in performance!
                </div>
            </div>
        </div>

        <div class='row vspace-top'>
            <div class="col-sm-3">
                <img src='img/SRNs.gif' class='img-fluid'>
            </div>

            <div class="col">
                <div class='paper-title'>
                    <a href="http://vsitzmann.github.io/srns/">Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations</a>

                </div>
                <div>
                    A continuous, 3D-structure-aware neural scene representation that encodes both geometry and appearance,
                    supervised only in 2D via a neural renderer, and generalizes for 3D reconstruction from a single posed 2D image.
                </div>
            </div>
        </div>

        <div class='row vspace-top'>
            <div class="col-sm-3">
                <img src='img/srn_seg_repimage.jpg' class='img-fluid'>
            </div>

            <div class="col">
                <div class='paper-title'>
                    <a href="https://www.computationalimaging.org/publications/semantic-srn/">Inferring Semantic Information with 3D Neural Scene Representations
                    </a>
                </div>
                <div>
                    We demonstrate that the features learned by neural implicit scene representations are useful for downstream
                    tasks, such as semantic segmentation, and propose a model that can learn to perform continuous 3D
                    semantic segmentation on a class of objects (such as chairs) given only a single, 2D (!) semantic label map!
                </div>
            </div>
        </div>

    <div class="section">
        <h2>Paper</h2>
        <hr>
        <div>
            <div class="list-group">
                <a href="https://arxiv.org/abs/2006.09661"
                   class="list-group-item">
                    <img src="img/paper_thumbnail.png" style="width:100%; margin-right:-20px; margin-top:-10px;">
                </a>
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
        <div class="bibtexsection">
            @inproceedings{sitzmann2019siren,
                author = {Sitzmann, Vincent
                          and Martel, Julien N.P.
                          and Bergman, Alexander W.
                          and Lindell, David B.
                          and Wetzstein, Gordon},
                title = {Implicit Neural Representations
                          with Periodic Activation Functions},
                booktitle = {Proc. NeurIPS},
                year={2020}
            }
        </div>
    </div>

    <hr>

    <footer>
        <p>Send feedback and questions to <a href="http://web.stanford.edu/~sitzmann/">Vincent Sitzmann</a></p>
    </footer>
</div>


<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>
